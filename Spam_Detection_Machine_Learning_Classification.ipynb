{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPNje9SwaVRRn6FDSDctVr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EsserMishelle/spam_vs_ham/blob/main/Spam_Detection_Machine_Learning_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spam Detection Machine Learning Classification\n",
        "\n",
        "## Problem Statement\n",
        "Email inboxes are inundated with unsolicited and low-value messages, making manual spam filtering inefficient and error-prone. This project applies supervised machine learning to automate the classification of email messages as spam or non-spam (ham). Because spam datasets are typically imbalanced, model evaluation emphasizes precision, recall, and F1-score rather than accuracy alone.\n"
      ],
      "metadata": {
        "id": "0REDkQkyppLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Loading and Preparaton"
      ],
      "metadata": {
        "id": "RrN52d6W0H6x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUm3X3E-sPWc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
        "# Copy the dataset for reference\n",
        "df_raw = df.copy()\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Empty columns are removed from the dataset. Only the text and labels columns are retained for modeling."
      ],
      "metadata": {
        "id": "5BCpbCLuy9bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename the columns as 'label' and 'message'\n",
        "if 'v1' in df.columns and 'v2' in df.columns:\n",
        "  df = df[['v1', 'v2']]\n",
        "  df.columns =['label', 'message']\n",
        "\n",
        "df.info()"
      ],
      "metadata": {
        "id": "K-0Cc5n-s8Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no null values in the label or message columns."
      ],
      "metadata": {
        "id": "pxSntWoLK1Ah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.  Explore Data Analysis"
      ],
      "metadata": {
        "id": "J_C2No20bDsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display((df['label'].value_counts()))\n",
        "df['label'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "sf1JuWphYgAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To better understand differences between spam and non-spam messages, message length is compared across classes.  \n",
        "The dataset is imbalanced (approximately 87% ham, 13% spam), so precision, recall, and F1-score are emphasized during evaluation.\n"
      ],
      "metadata": {
        "id": "m-3gflnn1joe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['word_count'] = df['message'].apply(lambda x: len(x.split()))\n",
        "df['word_count'].describe()\n"
      ],
      "metadata": {
        "id": "y11Iy9qMnsXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df.boxplot(column='word_count', by='label', grid = False, showfliers=False)\n",
        "plt.title('Message Length by Class')\n",
        "plt.suptitle('')\n",
        "plt.xlabel('Message Type')\n",
        "plt.ylabel('Word Count')"
      ],
      "metadata": {
        "id": "zFrTVsc91gsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spam messages tend to have longer average lengths than non-spam messages, while non-spam messages show greater variability.\n"
      ],
      "metadata": {
        "id": "f-mV9c4N33Zt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Text Preprocessing and Feature Engineering\n",
        "### Structure Cleaning/Label Cleaning"
      ],
      "metadata": {
        "id": "Hdf-kncmbqzi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spam messages are encoded as class 1 and non-spam (ham) messages as class 0 to support binary classification.\n"
      ],
      "metadata": {
        "id": "HWHuBAvWqM4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting labels to numberic. Spam will be class 1 and ham as class 0\n",
        "\n",
        "df['label_num'] = df['label'].map({'ham': 0, 'spam': 1})"
      ],
      "metadata": {
        "id": "hEvqff4cZzlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Message Cleaning/Normalization\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xbSp2wOgb_U_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "  text = re.sub(r'[^a-zA-Z0-9\\s]', '', text) # Remove non-alphabetic characters\n",
        "  text = re.sub(r'\\s+',' ', text).strip() # normalize whitespace\n",
        "  text = text.lower() # convert to lowercase\n",
        "  return text\n",
        "\n",
        "# Apply the clean_text function to the message column\n",
        "df['clean_message'] = df['message'].apply(clean_text)"
      ],
      "metadata": {
        "id": "o22oZLuYcbH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CORPUS Building"
      ],
      "metadata": {
        "id": "83ZlMuZZfHly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = df['clean_message'].tolist()"
      ],
      "metadata": {
        "id": "gL-xsZBLfGCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The corpus contains the cleaned message text used for feature extraction."
      ],
      "metadata": {
        "id": "XdBZEOixftSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorization\n",
        "\n",
        "Text messages are converted into numerical features using TF-IDF vectorization, the weighted Bag-of-Words representation, with unigrams and bigrams. Including bigrams allows the model to capture phrase-level patterns such as “free entry” or “claim now” that single words alone may miss."
      ],
      "metadata": {
        "id": "1wyBI3ILf2yK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_features = 3000)\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "X.shape"
      ],
      "metadata": {
        "id": "locH0MTqvXts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF vectorization transforms the corpus into a sparse numerical feature matrix. The result contains up to 3,000 weighted unigram and bigram features."
      ],
      "metadata": {
        "id": "VRY-v_RppJyx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.  Modeling and Evaluation\n",
        "\n",
        "Target Variable Definition\n"
      ],
      "metadata": {
        "id": "B3QRn8orqigZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['label_num']"
      ],
      "metadata": {
        "id": "st2VwRPx5hU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train-Test-Split"
      ],
      "metadata": {
        "id": "WOuCtE6s4yts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2,\n",
        "                                                    random_state=42, stratify=y)\n",
        "# Stratifcation preserves spam/ham imbalance ratio"
      ],
      "metadata": {
        "id": "7W8Q0p9V4x9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Metrics\n",
        "\n",
        "Because misclassifying emails has different consequences, the following metrics are the focus of this project:\n",
        "\n",
        "- **Precision**: Of the emails flagged as spam, how many were actually spam?\n",
        "  - High precision minimizes false positives (valid emails sent to spam).\n",
        "\n",
        "- **Recall**: Of all spam emails, how many were correctly detected?\n",
        "  - High recall minimizes false negatives (spam reaching the inbox).\n",
        "\n",
        "- **False Positives (FP)**: Valid emails incorrectly flagged as spam.\n",
        "- **False Negatives (FN)**: Spam emails incorrectly labeled as valid.\n"
      ],
      "metadata": {
        "id": "cg-5OXANDA77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Model (Majority Class)"
      ],
      "metadata": {
        "id": "xEXWid9M-haE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline model\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# The baseline model predicts the majority class by assigning a value of 0 (ham)\n",
        "# to every test instance using np.zeros_like(y_test)\n",
        "y_pred = np.zeros_like(y_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "vjSolpd9-vHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Warnings appear because the baseline model predicts only the majority class (ham), resulting in undefined precision for spam."
      ],
      "metadata": {
        "id": "d5xLXVFQXj2V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression (default parameters)"
      ],
      "metadata": {
        "id": "fMY1GDGY2RGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# logistic regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred = lr.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "YGuHXYJK_aDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Label Encoding:**  \n",
        "- 0 = ham  \n",
        "- 1 = spam"
      ],
      "metadata": {
        "id": "py4DAkxYVAKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cnfusion Matrices:**\n",
        "- Top-right cell: **False Positives** (ham incorrectly flagged as spam)\n",
        "- Bottom-left cell: **False Negatives** (spam messages missed in prediction)\n",
        "\n"
      ],
      "metadata": {
        "id": "13P_vD-t1QEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "fig, ax = plt.subplots(figsize=(7, 5))\n",
        "\n",
        "im = ax.imshow(cm, cmap='Accent', interpolation='nearest')\n",
        "fig.colorbar(im, ax=ax)\n",
        "\n",
        "ax.set_xticks([0, 1])\n",
        "ax.set_yticks([0, 1])\n",
        "ax.set_xticklabels(['ham', 'spam'])\n",
        "ax.set_yticklabels(['ham', 'spam'])\n",
        "\n",
        "ax.set_xlabel('Predicted label')\n",
        "ax.set_ylabel('Atual label')\n",
        "ax.set_title('Confusion Matrix (Logistic Regression)')\n",
        "\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        ax.text(j, i, cm[i, j], ha='center', va='center')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hpJgzsvUNRoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Interpretation:**\n",
        "\n",
        "Logistic Regression provides a strong linear baseline with high overall accuracy and good spam recall, while remaining interpretable and well-calibrated.\n",
        "\n",
        "*Precision* reflects how often messages predicted as spam are truly spam, *recall* measures how many actual spam messages are identified, and the *F1-score* balances these tow metrics to account for class imbalance."
      ],
      "metadata": {
        "id": "QCGjCTfa83gp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest (default parameters)\n"
      ],
      "metadata": {
        "id": "iBMd-RBG2JYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the baseline Random Forest model.\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "PE5R5-xt_3VX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "fig, ax = plt.subplots(figsize=(7, 5))\n",
        "\n",
        "im = ax.imshow(cm, cmap='Accent', interpolation='nearest')\n",
        "fig.colorbar(im, ax=ax)\n",
        "\n",
        "ax.set_xticks([0, 1])\n",
        "ax.set_yticks([0, 1])\n",
        "ax.set_xticklabels(['ham', 'spam'])\n",
        "ax.set_yticklabels(['ham', 'spam'])\n",
        "\n",
        "ax.set_xlabel('Predicted label')\n",
        "ax.set_ylabel('Actual label')\n",
        "ax.set_title('Confusion Matrix (Random Forest)')\n",
        "\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        ax.text(j, i, cm[i, j], ha='center', va='center')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XJpSWPSzL5o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Interpretation:**\n",
        "\n",
        "The Random Forest improves overall performance by aggregating multiple decision trees, achieving high precision and strong recall for spam detection."
      ],
      "metadata": {
        "id": "J3gAPXNF9HZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Model                | False Positives | False Negatives |\n",
        "|----------------------|-----------------|-----------------|\n",
        "| Logistic Regression  | 0               | 38              |\n",
        "| Random Forest        | 1               | 24              |"
      ],
      "metadata": {
        "id": "t2TdkwoIk81x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compared to Logistic Regression, the Random Forest model detects 14 additional spam messages but incorrectly flags 1 non-spam messages."
      ],
      "metadata": {
        "id": "DH6EJIp3lUDh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ROC and AUC Charts for Spam and Ham Classification\n",
        "\n",
        "ROC and AUC charts are used to evaluate the performance among the 2 models in separating spam from ham, showing the trade-offs between true positive and false positive rates at various threshold settings."
      ],
      "metadata": {
        "id": "c01CJLePvlxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "proba_lr = lr.predict_proba(X_test)[:,1]\n",
        "proba_rf = rf.predict_proba(X_test)[:,1]\n",
        "\n",
        "fpr_lr, tpr_lr, _ = roc_curve(y_test, proba_lr)\n",
        "fpr_rf, tpr_rf, _ = roc_curve(y_test, proba_rf)\n",
        "\n",
        "auc_lr = roc_auc_score(y_test, proba_lr)\n",
        "auc_rf = roc_auc_score(y_test, proba_rf)\n",
        "\n",
        "plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {auc_lr:.4f})')\n",
        "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {auc_rf:.4f})')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', label='Random')\n",
        "plt.legend()\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve Comparison')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tEBsVyyOHXaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ROC-AUC Interpretation:**\n",
        "\n",
        "While the Random Forest model achieves higher recall and F1-score at the default classification threshold (as reflected in the confusion matrices), both models perform very well overall, as shown by their high ROC–AUC values. This suggests that much of the distinction between spam and ham can be explained by a simple linear model.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J7NpbhgUol2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Threshold Tuning (Decision Tradeoffs)\n",
        "\n",
        "By default, classification models use a probability threshold of 0.5 to assign class labels.\n",
        "However, this threshold may not be optimal for all real-world use cases.\n",
        "\n",
        "In spam detection, lowering the threshold can increase spam recall at the cost of more false positives,\n",
        "while raising the threshold prioritizes precision by reducing the chance of blocking legitimate emails.\n",
        "\n",
        "The interactive slider below demonstrates how adjusting the decision threshold impacts model performance."
      ],
      "metadata": {
        "id": "mSFLNGqHE-ov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output"
      ],
      "metadata": {
        "id": "7rjxxf7l5ygF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_proba = lr.predict_proba(X_test)[:, 1]"
      ],
      "metadata": {
        "id": "tXR_f3KU6Dsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_threshold(threshold):\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    # Apply threshold\n",
        "    y_pred_thresh = (y_proba >= threshold).astype(int)\n",
        "\n",
        "    # Metrics\n",
        "    precision = precision_score(y_test, y_pred_thresh)\n",
        "    recall = recall_score(y_test, y_pred_thresh)\n",
        "    f1 = f1_score(y_test, y_pred_thresh)\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred_thresh)\n",
        "\n",
        "    print(f\"Threshold: {threshold:.2f}\")\n",
        "    print(f\"Precision: {precision:.3f}\")\n",
        "    print(f\"Recall:    {recall:.3f}\")\n",
        "    print(f\"F1-score:  {f1:.3f}\\n\")\n",
        "\n",
        "    # plt.figure(figsize=(6, 4))\n",
        "    plt.imshow(cm, cmap='Accent', interpolation='nearest')\n",
        "    plt.colorbar()\n",
        "    plt.xticks([0, 1], ['ham', 'spam'])\n",
        "    plt.yticks([0, 1], ['ham', 'spam'])\n",
        "\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('Actual Label')\n",
        "\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            plt.text(j, i, cm[i, j], ha='center', va='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "JrF80v0g6j4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold_slider = widgets.FloatSlider(\n",
        "    value=0.5,\n",
        "    min=0.1,\n",
        "    max=0.9,\n",
        "    step=0.05,\n",
        "    description='Threshold:',\n",
        "    continuous_update=False\n",
        ")\n",
        "\n",
        "widgets.interact(update_threshold, threshold=threshold_slider)\n"
      ],
      "metadata": {
        "id": "BTYX3s1p9YTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:**  \n",
        "Lowering the threshold increases spam recall but introduces more false positives,\n",
        "while higher thresholds improve precision at the risk of missing spam messages.\n",
        "\n",
        "This highlights the importance of selecting a threshold that aligns with business priorities\n",
        "rather than relying solely on the default value.\n"
      ],
      "metadata": {
        "id": "nJcHwh-HFOJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Exploratory Optional Extensions"
      ],
      "metadata": {
        "id": "w1Qtvuq_rR_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of Words (Baseline - Exploratory)"
      ],
      "metadata": {
        "id": "WcIZp9Jv8gGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "bow = CountVectorizer(max_features=3000, stop_words='english')\n",
        "\n",
        "X_bow = bow.fit_transform(corpus)\n",
        "Xb_train, Xb_test, yb_train, yb_test = train_test_split(X_bow, y, test_size=.25,\n",
        "                                                    random_state=42, stratify=y)\n",
        "lr_bow = LogisticRegression()\n",
        "# lr_bow = LogisticRegression(max_iter=1000)\n",
        "lr_bow.fit(Xb_train, yb_train)\n",
        "\n",
        "yb_pred = lr_bow.predict(Xb_test)\n",
        "print(classification_report(yb_test, yb_pred))"
      ],
      "metadata": {
        "id": "V8592TB78iy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an exploratory comparison, a Bag-of-Words (raw word counts) representation is evaluated using Logistic Regression. Although Bag-of-Words achieves slightly higher spam recall on this dataset, its dependence on raw term frequency makes it more sensitive to dataset-specific words. TF-IDF is selected as the preferred representation because it provides greater robustness to various language patterns and better generalization on the dataset."
      ],
      "metadata": {
        "id": "FPZYP-ZB_A31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer-Based Model (BERT / Hugging Face – Exploratory)"
      ],
      "metadata": {
        "id": "nfdgedA-j7_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "bert_classifier = pipeline(\n",
        "    'text-classification',\n",
        "    model = 'distilbert-base-uncased-finetuned-sst-2-english',\n",
        "    tokenizer = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
        ")\n",
        "\n",
        "sample_text = df['message'].sample(10, random_state=42).tolist()\n",
        "bert_predictions = bert_classifier(sample_text)\n",
        "list(zip(sample_text, bert_predictions)\n",
        ")"
      ],
      "metadata": {
        "id": "5VDqmZrAB6Tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformer-Based Model (Exploratory):**\n",
        "\n",
        "A pretrained DistilBERT model from Hugging Face is explored using the transformers pipeline to explore transformer-based NLP models. The model is fine-tuned for sentiment analysis rather than spam classification, so results are not directly comparable to supervised models trained on the dataset. Due to this mismatch and higher computational cost, classical TF-IDF–based models remain the preferred solution for this task."
      ],
      "metadata": {
        "id": "8nAbgmiaD_j0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. Final Summary\n",
        "This project evaluates multiple supervised learning approaches for spam classification using TF-IDF text features with an imbalance class distribution. A majority-class baseline demonstrates that accuracy alone is insufficient, prompting the need for precision, recall, F1-score, and ROC–AUC for evaluation.\n",
        "\n",
        "Logistic Regression and Random Forest models both achieve strong performance. Logistic Regression provides a stable and interpretable linear baseline with reliable probability estimates, while the default Random Forest model achieves higher precision and strong recall by aggregating multiple decision trees. Excessive regularization of the Random Forest leads to underfitting and reduced spam recall, illustrating the importance of balancing model complexity with dataset characteristics.\n",
        "\n",
        "Logistic Regression serves as a strong and interpretable baseline. However, final model selection depends on the relative cost of false positives versus false negatives, which is addressed in the Recommendation section.\n",
        "\n"
      ],
      "metadata": {
        "id": "jdZF64EsEzRT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7. Recommendation\n",
        "Alternative feature representations, including bigrams and Bag-of-Words, produce results comparable to unigram TF-IDF, indicating stable model behavior and the keyword-driven nature of SMS spam. A pretrained BERT-based model from Hugging Face is explored to gain exposure to modern NLP techniques, but does not provide a clear advantage for this short-text classification task due to higher computational cost and the lack of task-specific fine-tuning.\n",
        "\n",
        "**For spam filtering applications where false positives are more costly than false negatives, the default Random Forest model is preferred due to its high precision and robust performance.** Logistic Regression remains a strong alternative when interpretability and simplicity are prioritized."
      ],
      "metadata": {
        "id": "j6eTcm8Qc7WX"
      }
    }
  ]
}